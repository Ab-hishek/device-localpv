---
- block:
    - name: Get the list of nodes from the value of env's in run_e2e_test.yml file
      set_fact:
        node_list: "{{ node_names.split(',') }}"
      when: "node_names != ''"

    - name: Get the list of all those nodes which are in Ready state and having no taints in cluster
      shell: > 
        kubectl get nodes -o json | jq -r 'try .items[] | select(.spec.taints|not)
        | select(.status.conditions[].reason=="KubeletReady" and .status.conditions[].status=="True")
        | .metadata.name'
      register: schedulabel_nodes
      when: "node_names == ''"

    - block:
    
        - name: Label the nodes for privileged DaemonSet pods to schedule on it
          shell: >
            kubectl label node {{ item }} test=device-partition
          args:
            executable: /bin/bash
          register: label_status
          failed_when: "label_status.rc != 0"
          with_items: "{{ node_list }}"

        - name: Update the DaemonSet yaml to use nodes label selector
          shell: >
            sed -i -e "s|#nodeSelector|nodeSelector|g" \
            -e "s|#test: device-partition|test: device-partition|g" /e2e-tests/experiments/device-localpv-provisioner/device_partition_ds.yml
          args:
            executable: /bin/bash
          register: status
          failed_when: "status.rc != 0"

      when: "node_names != ''"

    - name: Create a DaemonSet with privileged access for creation of device meta-partition on nodes
      shell: >
        kubectl apply -f /e2e-tests/experiments/device-localpv-provisioner/device_partition_ds.yml
      args:
        executable: /bin/bash
      register: status
      failed_when: "status.rc != 0"

    - name: Check if DaemonSet pods are in running state on all desired nodes
      shell: >
        kubectl get pods -n e2e -l app=device-partition
        --no-headers -o custom-columns=:.status.phase | sort | uniq
      args:
        executable: /bin/bash
      register: result
      until: "result.stdout == 'Running'"
      delay: 3
      retries: 40

    - name: Get the list of DaemonSet pods
      shell: >
        kubectl get pods -n e2e -l app=device-partition --no-headers
        -o custom-columns=:.metadata.name
      args:
        executable: /bin/bash
      register: ds_pods_list

    - name: Create meta partition on disk {{ pv_disk }} on desired worker nodes
      shell: >
        kubectl exec -ti {{ item }} -- bash
        -c 'parted {{ pv_disk }} mklabel gpt && parted {{ pv_disk }} mkpart {{ device_name }} 1MiB 10MiB'
      args:
        executable: /bin/bash
      register: zpool_status
      failed_when: "zpool_status.rc != 0"
      with_items: "{{ ds_pods_list.stdout_lines }}"

  always:

    # Here always block tasks will execute everytime irrespective of previous tasks result
    # so here we will delete daemonset pods and remove label which were created on nodes.
    # Here purpose for using `ignore_errors: true` is that if this test fails even before
    # creating daemonset or labeling the node then deleting them will fail as they don't exist.
    
  - name: Delete the DaemonSet
    shell: >
      kubectl delete -f /e2e-tests/experiments/device-localpv-provisioner/device_partition_ds.yml
    args:
      executable: /bin/bash
    register: status
    failed_when: "status.rc != 0"
    ignore_errors: true

  - name: Remove the label from nodes
    shell: >
      kubectl label node {{ item }} test-
    args:
      executable: /bin/bash
    register: label_status
    failed_when: "label_status.rc != 0"
    with_items: "{{ node_list }}"
    when: "node_names != ''"
    ignore_errors: true

  - name: Remove the label from nodes
    shell: >
      kubectl label node {{ item }} test-
    args:
      executable: /bin/bash
    register: label_status
    failed_when: "label_status.rc != 0"
    with_items: "{{ schedulabel_nodes.stdout_lines }}"
    when: "node_names == ''"
    ignore_errors: true